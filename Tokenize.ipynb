{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdUV2Wq1fx8dJ8XOz81AE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/logicaltrojan/2018_hcpc/blob/master/Tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOjcI8-WR8v8",
        "outputId": "0b0b4d6c-5b10-4201-bbb8-dbb02cfd76b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workd Tokenize:  ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "Workd Punct:  ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "Keras Text to word sequence [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "print(\"Workd Tokenize: \" ,word_tokenize(\"Don't be fooled by the dark sounding name , Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
        "# 구두점 별도 분리\n",
        "print(\"Workd Punct: \" ,WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name , Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
        "# 모든 알파벳 소문자, apstrp preserve,\n",
        "print(\"Keras Text to word sequence\", text_to_word_sequence(\"Don't be fooled by the dark sounding name , Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why we need delicate algorithm for tokenizing? \n",
        "\n",
        "- it is not just splitting words\n",
        "\n",
        "1) 구두점이나 특수문자를 단순 제외해서는 안된다. \n",
        "- 마침표의 경우 문장의 경계를 알수 있는데 도움을 주므로 마침표를 제외한지 않는 경우가 있다. \n",
        "- 단어 자체에 구두점을 가지고 있는 경우 Ph.d , AT&T 와 같은 특수문자, 01/02/06 처럼 날짜 표기, $45.55 같은 가격 표기의 경우 하나로 취급하고 싶을 수 있다. \n",
        "- 숫자 사이에 , 가 들어가는 경우 \n",
        "\n",
        "2) 줄임말과 단어 내에 띄어쓰기가 있는 경우 \n",
        "- apstrp -> 압축된 단어가 있을 수 있음 what're -> what are we're we are\n",
        "- re -> 접어 (clitic)이라고 한다 .\n",
        "- New York , rock'n'roll 하나의 단어지만 중간에 띄어쓰기가 존재 , 즉 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야하는 경우도 있을 수 있으므로, 토큰화 작업은 저런 단어를 하나로 인식할 수 있는 능력을 가져야 함\n",
        "\n",
        "3) 표준 토큰화 예제\n",
        "\n",
        "### Penn Treebank Toekenization \n",
        "\n",
        "1) include hypen words\n",
        "\n",
        "2) doesn't 와 같이 apstr로 clitic 이 있는 단어는 분리 \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZIflS8YATtBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "# home-base , dosen't \n",
        "\n",
        "print(\"tokens :\", tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMoIdXl3V1cz",
        "outputId": "9f754363-e847-4393-cc3b-18117ec15f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Sentence Tokenization (문장 토큰화) \n",
        "\n",
        "- 토큰이 단위가 문장일 경우\n",
        "- Corpus 내에서 문장단위로 구분하는 작업 (sentence segmenation)\n",
        "- 통통 corpus내에서 문장단위로 구분되어 있지 않아서 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요 \n",
        "- ?/./!로 구분할 수 있지만, (.) 가 애매함\n",
        "\n",
        "EX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com 로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\n",
        "\n",
        "EX2) Since I’m actively looking for Ph.D. students, I get the same question a dozen times every year.\n",
        "\n",
        "\n",
        "NLTK -> 영어문장의 toeken화를 수행하는 sent_tokenize를 지원 \n"
      ],
      "metadata": {
        "id": "E75jQ0NRXlCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text =  \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "\n",
        "print(sent_tokenize(text))\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text))\n",
        "# sorted out "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MKzfgHpYfK2",
        "outputId": "676a53e1-c474-4430-b84a-8dd7e03e3c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n",
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어의 경우 KSS \n",
        "\n",
        "import kss\n",
        "\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어 로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "\n",
        "print(kss.split_sentences(text))\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "\n",
        "print(kss.split_sentences(text))\n",
        "\n",
        "#한영 섞여 있어도 잘 된다\n",
        "text = 'Deep Learning 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어(e.x. korean) 로 할 때 너무 어렵습니다. ph.d kim, 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3h2dmJXZvjy",
        "outputId": "09dc60d0-6cbe-45d9-ab6a-2cd595d94854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.8/dist-packages (4.5.1)\n",
            "Requirement already satisfied: emoji==1.2.0 in /usr/local/lib/python3.8/dist-packages (from kss) (1.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from kss) (3.0)\n",
            "Requirement already satisfied: pecab in /usr/local/lib/python3.8/dist-packages (from kss) (1.0.8)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from pecab->kss) (3.6.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->pecab->kss) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pecab->kss) (22.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->pecab->kss) (57.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pecab->kss) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pecab->kss) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytest->pecab->kss) (1.15.0)\n",
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어 로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n",
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n",
            "['Deep Learning 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어(e.x. korean) 로 할 때 너무 어렵습니다.', 'ph.d kim, 이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한국어는 토큰화가 어렵다 \n",
        "\n",
        "- 영어는 어절로 하는 띄어쓰기 기준 토큰화를 수행해도 잘된다. 단어 단위의 최소 의미단위를 가지기 때문\n",
        "- 한국어는 안됨 \n",
        "- 어절 토큰화는 지양한다. 교착어의 특성 때문 \n",
        "\n",
        "1) 교착어의 특성 \n",
        "- 조사가 존재 (그는, 그와 , 그가 .. )\n",
        "- 서로 같은 단어임에도 조사각 붙어서 다른 단어로 인식이 되면 자연어 처리가 힘듬 \n",
        "- 형태소 (morpheme) 가장 작은 말의 단위 \n",
        "  - 자립 형태소 : 접사 / 어미 / 조사와 자립하여 사용할 수 있는 형태소, 그 자체가 단어 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탁사 \n",
        "  - 의존 형태소 : 다른 형태와 결합하여 사용 (접사, 어미, 조사, 어간) \n",
        "\n",
        "> 에디가 책을 읽었다 \n",
        "\n",
        "어절로 나눌 경우 [\"에디가\", \"책을\" \"읽었다\"]\n",
        "\n",
        "형태소 단위 \n",
        "- 자립 : 에디, 책\n",
        "- 의존 : 가, 을 ,읽, 었, 다 -> 하나의 단위로 모름 \n",
        "\n",
        "2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\n",
        "\n",
        "- 띄어쓰기 잘못해도 잘 소통되는 언어.. (좋은 설계지만.. )\n",
        "- 영어는 띄어쓰기가 잘 지켜짐, 띄어쓰지 않으면 소통이 힘들기 때문\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M_D5cMcsb5ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Part-of-speech tagging (품사 태깅) \n",
        "\n",
        "- 단어의 의미를 제대로 파악하기 위해서, 해당 단어가 어떤 품사로 쓰였는지 보는 것 \n",
        "- 못 (명사) -> 망치를 사용해서 목재를 고정하는 물건 , 못(부사) -> 동사 앞에서 할 수 없다는 의미 \n"
      ],
      "metadata": {
        "id": "-LB-0Iz9d31k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "tokenize_sentence = word_tokenize(text)\n",
        "\n",
        "print ( \"tokenize\", tokenize_sentence)\n",
        "print (\"tagging\", pos_tag(tokenize_sentence));\n",
        "# PRP : 인칭 대명사 / VBP 동사 / RB 부사 / VBG 현재 부사 / IN 전치사 / NNP 고유 명사 / NNS 복수형 명사 / CC 접속사\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWoCz_Szeh0d",
        "outputId": "34f76047-e299-4cfc-f17a-2549d4965523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "tagging [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- 한국어 tagging 에는 konlpy package 사용 가능 \n",
        "- okt(open korea text), meacab, komoran, hannanum, kkma\n",
        "- tagging 에 쓰인 morpheme analyzer에 따라 morpheme이 다르게 나올 수 있어서 각 형태소 분석기가 어떻게 나누는지 특성을 이해하고 사용하면 좋을 듯"
      ],
      "metadata": {
        "id": "orIPWHZCtLJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "print(okt.morphs(text))\n",
        "print(okt.pos(text))\n",
        "print(okt.nouns(text))\n",
        "\n",
        "print(kkma.morphs(text)) \n",
        "# kkma 한 <-> 하+ ㄴ  에는 <-> 에+ 는 가봐요/가보 + 아요ㅓㅓ \n",
        "print(kkma.pos(text))\n",
        "print(kkma.nouns(text))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWPOq4fwtc8e",
        "outputId": "b9c1bf72-1a76-4622-a4a7-380cb4194001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "['코딩', '당신', '연휴', '여행']\n",
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    }
  ]
}